{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e227b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run on actual code\n",
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b9ea0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import MetricFrame, selection_rate, false_positive_rate, true_positive_rate\n",
    "from fairlearn.metrics import demographic_parity_difference, demographic_parity_ratio, equalized_odds_difference\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n",
    "import sys\n",
    "sys.path.append(\"./subjects/\")\n",
    "import LogisticRegression\n",
    "import Decision_Tree_Classifier\n",
    "import TreeRegressor\n",
    "import Discriminant_Analysis\n",
    "import SVM\n",
    "\n",
    "from adf_utils.config import census, credit, bank, compas\n",
    "from adf_data.census import census_data\n",
    "from adf_data.credit import credit_data\n",
    "from adf_data.bank import bank_data\n",
    "from adf_data.compas import compas_data\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "def getMostAccurateConfig(model,dataset,sensitive_feature,algo):\n",
    "    df = pd.read_csv(\"./Dataset\" + \"/\" + f\"{model}_{dataset}_{sensitive_feature}_{algo}.csv\")\n",
    "    return df[df['score'] == df['score'].max()].iloc[0].inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b280b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [(\"LR\", \"LogisticRegression\"),(\"RF\",\"TreeRegressor\"),(\"SV\",\"SVM\"),(\"DT\",\"Decision_Tree_Classifier\")]\n",
    "datasets = [(\"census\", \"gender\",9), (\"census\", \"race\",8), (\"credit\", \"gender\",9), (\"bank\",\"age\",1), (\"compas\",\"gender\",1), (\"compas\",\"race\",3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "965a564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized_moment = EqualizedOdds(difference_bound=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86527955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data(dataset):\n",
    "    data = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data, \"compas\": compas_data}\n",
    "    data_config = {\"census\":census, \"credit\":credit, \"bank\":bank, \"compas\": compas}\n",
    "    X, Y, input_shape, nb_classes = data[dataset]()\n",
    "    Y = np.argmax(Y, axis=1)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)\n",
    "    return train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903aa852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1, 0.03141566502549982, 75.0624694227447, 0, 5.31657081542493, 1015, 2, 0, 0, 1, 1, 1, 1]\n",
      "['lbfgs', 'elasticnet', False, 0.0001, 1.0, False, 1.0, 950, 'ovr', 'none', 'None', 'None', 0, True, 'int', '4', 'int', '1', 'int', '5', 'int', '6', 'int', '6', 'int', '5', 'int', '0', 'int', '2', 'int', '5', 'int', '4', 'int', '9', 'int', '9', 'int', '8', 'int', '2', 'int', ',', 'int', ' ', 'int', '7', 'int', '5', 'int', '.', 'int', '0', 'int', '6', 'int', '2', 'int', '4', 'int', '6', 'int', '9', 'int', '4', 'int', '2', 'int', '2', 'int', '7', 'int', '4', 'int', '4', 'int', '7', 'int', ',', 'int', ' ', 'int', '0', 'int', ',', 'int', ' ', 'int', '5', 'int', '.', 'int', '3', 'int', '1', 'int', '6', 'int', '5', 'int', '7', 'int', '0', 'int', '8', 'int', '1', 'int', '5', 'int', '4', 'int', '2', 'int', '4', 'int', '9', 'int', '3', 'int', ',', 'int', ' ', 'int', '1', 'int', '0', 'int', '1', 'int', '5', 'int', ',', 'int', ' ', 'int', '2', 'int', ',', 'int', ' ', 'int', '0', 'int', ',', 'int', ' ', 'int', '0', 'int', ',', 'int', ' ', 'int', '1', 'int', ',', 'int', ' ', 'int', '1', 'int', ',', 'int', ' ', 'int', '1', 'int', ',', 'int', ' ', 'int', '1', 'int', ']', 'int']\n",
      "['lbfgs', 'l2', False, 0.0001, 1.0, False, 1.0, 950, 'ovr', None, None, 2019, 0, True, None, '4', 'int', '1', 'int', '5', 'int', '6', 'int', '6', 'int', '5', 'int', '0', 'int', '2', 'int', '5', 'int', '4', 'int', '9', 'int', '9', 'int', '8', 'int', '2', 'int', ',', 'int', ' ', 'int', '7', 'int', '5', 'int', '.', 'int', '0', 'int', '6', 'int', '2', 'int', '4', 'int', '6', 'int', '9', 'int', '4', 'int', '2', 'int', '2', 'int', '7', 'int', '4', 'int', '4', 'int', '7', 'int', ',', 'int', ' ', 'int', '0', 'int', ',', 'int', ' ', 'int', '5', 'int', '.', 'int', '3', 'int', '1', 'int', '6', 'int', '5', 'int', '7', 'int', '0', 'int', '8', 'int', '1', 'int', '5', 'int', '4', 'int', '2', 'int', '4', 'int', '9', 'int', '3', 'int', ',', 'int', ' ', 'int', '1', 'int', '0', 'int', '1', 'int', '5', 'int', ',', 'int', ' ', 'int', '2', 'int', ',', 'int', ' ', 'int', '0', 'int', ',', 'int', ' ', 'int', '0', 'int', ',', 'int', ' ', 'int', '1', 'int', ',', 'int', ' ', 'int', '1', 'int', ',', 'int', ' ', 'int', '1', 'int', ',', 'int', ' ', 'int', '1', 'int', ']', 'int']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n",
      "/home/normenyu/anaconda3/lib/python3.9/site-packages/fairlearn/reductions/_moments/utility_parity.py:251: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  lambda_event = (lambda_vec[\"+\"] - self.ratio * lambda_vec[\"-\"]).sum(level=_EVENT) / \\\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    for d in datasets:\n",
    "        most_accurate = getMostAccurateConfig(m[0],d[0],d[1],\"mutation\")\n",
    "        if(m[1] == \"LogisticRegression\"):\n",
    "            input_program = input_program = LogisticRegression.logistic_regression\n",
    "        elif(m[1] == \"Decision_Tree_Classifier\"):\n",
    "            input_program = Decision_Tree_Classifier.DecisionTree\n",
    "        elif(m[1] == \"TreeRegressor\"):\n",
    "            input_program = TreeRegressor.TreeRegress\n",
    "        elif(m[1] == \"SVM\"):\n",
    "            input_program = SVM.SVM\n",
    "        X_train, X_test, y_train, y_test = get_data(d[0])\n",
    "        \n",
    "        # I only really need to use the parser for the most accurate 'inp'\n",
    "        # Exponentiated Gradient will retrain everything so the fit that this does will be \"wasted\"\n",
    "        _, model, _, _, _, _ = input_program(most_accurate,X_train, X_test, y_train, y_test, d[2])\n",
    "        training_model = ExponentiatedGradient(model,equalized_moment)\n",
    "        training_model.fit(X_train,y_train,sensitive_features=X_train[:,d[2]-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5006ba08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
